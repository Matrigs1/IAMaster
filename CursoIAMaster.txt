Large Language Models (LLM's): Modelos de inteligência artificial generativa, com alta escala de processamento de dados. Feitos para interação com seres humanos. Ex: ChatGPT, Lhama, Claude, Gemini.

A OpenIA desenvolveu uma biblioteca dentro do Python que facilita a integração com sistemas, seu nome é 'openai'.

Nela é feita a autenticação do usuário com a API Key, depois, montamos um 'client' com o nosso tipo de retorno de preferência, junto com o 'model' desejado, a 'role' de quem estará fazendo o prompt e, o 'content', que é o conteúdo em si do prompt.

Se a propriedade stream não for passada como True, a resposta será aguardada e mostrada completamente. Caso contrário, é possível mostrar a resposta em pequenas partes, assim como é feito no ChatGPT.

Além da 'role' como user, podemos também passar como sistema, fornecendo um contexto para a Inteligência Artificial. Em sistema, podemos pedir por uma expertise ou, passar dados de uma base, para que as respostas do 'Agent', sejam agora com base nesses dados.

# Atributos customizados
max_tokens = 150  # Limita a quantidade de tokens retornados pela resposta.
temperature 0.2 = # Quanto maior a temperatura, mais randômica será a resposta. Para menores temperaturas, a resposta será mais direta.

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
LangChain: Um framework/ecossistema para desenvolvimento de aplicações de inteligência artificial. Facilita e possibilita novos recursos para interação com LLM's e criação de aplicações com elas.

langchain_openai = É uma biblioteca da langchain integrada com a openai. Fornece abstração e facilidades na utilização da IA.

Recursos de cache: 
InMemoryCache() = Esse cache vai ficar em memória enquanto a aplicação estiver rodando. Se a execução parar, ele é perdido. Mas de todo modo, serve para minimizar perguntas e respostas repetidas e otimizar o custo do modelo.
SQLiteCache = Esse tipo de cache irá persistir a resposta em um arquivo .db e, sempre que for chamado, independente se a aplicação for interrompida, buscará o prompt já enviado. Ex:
set_llm_cache(
    SQLiteCache(
        database_path='openai_cache.db'
    )
)
